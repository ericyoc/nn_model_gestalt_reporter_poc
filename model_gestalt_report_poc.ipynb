{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPx9E35v2OOshuomPl+U9mJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "12fecdb6d4694f9c899c61190f6ee35e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1f225c70604464ba2051a75fc7efbf2",
              "IPY_MODEL_f187d232e3e24e76a0882b9242d45ffd",
              "IPY_MODEL_6c0a04cd1b7941acb1f76955a2e472b7"
            ],
            "layout": "IPY_MODEL_0f896be730a8490dab7fe34796726d7b"
          }
        },
        "b1f225c70604464ba2051a75fc7efbf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3c2bcaa655a4b62aa2f1dc984a31fd3",
            "placeholder": "​",
            "style": "IPY_MODEL_c30233db24f4459ca6056ab67f9d22b5",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f187d232e3e24e76a0882b9242d45ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90442643e73a4b58b9a9f70fe411d128",
            "max": 267844284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52d47ed18760446086a182b07ad2493f",
            "value": 267844284
          }
        },
        "6c0a04cd1b7941acb1f76955a2e472b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8730ff89118a4cdfaac7acf6c9cc9d99",
            "placeholder": "​",
            "style": "IPY_MODEL_d519d9e127434a4c8472507ab149d756",
            "value": " 268M/268M [00:03&lt;00:00, 102MB/s]"
          }
        },
        "0f896be730a8490dab7fe34796726d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c2bcaa655a4b62aa2f1dc984a31fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c30233db24f4459ca6056ab67f9d22b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90442643e73a4b58b9a9f70fe411d128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52d47ed18760446086a182b07ad2493f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8730ff89118a4cdfaac7acf6c9cc9d99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d519d9e127434a4c8472507ab149d756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericyoc/nn_model_gestalt_reporter_poc/blob/main/model_gestalt_report_poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install huggingface_hub requests\n",
        "\n",
        "# Optional but recommended for the full script\n",
        "#!pip install pathlib2"
      ],
      "metadata": {
        "id": "2U917Bf2aHzP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Hugging Face Model Downloader for Reverse Engineering\n",
        "Downloads different types of models to /content/ for analysis\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from pathlib import Path\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def install_requirements():\n",
        "    \"\"\"Install required packages\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = ['huggingface_hub', 'requests']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            __import__(package.replace('-', '_'))\n",
        "            print(f\"[OK] {package} already installed\")\n",
        "        except ImportError:\n",
        "            print(f\"[INSTALL] Installing {package}...\")\n",
        "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '-q'])\n",
        "\n",
        "class HuggingFaceModelDownloader:\n",
        "    def __init__(self, download_dir=\"/content\"):\n",
        "        self.download_dir = Path(download_dir)\n",
        "        self.download_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Curated list of different model types for reverse engineering\n",
        "        self.model_catalog = {\n",
        "            'image_classification': {\n",
        "                'description': 'Image Classification Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'microsoft/resnet-50',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'resnet50_imagenet.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'ImageNet',\n",
        "                        'size': 'Medium'\n",
        "                    },\n",
        "                    {\n",
        "                        'repo_id': 'google/vit-base-patch16-224',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'vit_base_imagenet.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'ImageNet',\n",
        "                        'size': 'Large'\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            'text_classification': {\n",
        "                'description': 'Text Classification Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'twitter_sentiment.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'Twitter',\n",
        "                        'size': 'Medium'\n",
        "                    },\n",
        "                    {\n",
        "                        'repo_id': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'distilbert_sst2.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'SST-2',\n",
        "                        'size': 'Small'\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            'object_detection': {\n",
        "                'description': 'Object Detection Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'microsoft/table-transformer-detection',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'table_detection.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'Custom',\n",
        "                        'size': 'Medium'\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            'onnx_models': {\n",
        "                'description': 'ONNX Format Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'onnx/models',\n",
        "                        'filename': 'vision/classification/mnist/model/mnist-8.onnx',\n",
        "                        'local_name': 'mnist_digit_classifier.onnx',\n",
        "                        'format': 'onnx',\n",
        "                        'dataset': 'MNIST',\n",
        "                        'size': 'Small'\n",
        "                    },\n",
        "                    {\n",
        "                        'repo_id': 'onnx/models',\n",
        "                        'filename': 'vision/classification/resnet/model/resnet50-v1-7.onnx',\n",
        "                        'local_name': 'resnet50_imagenet.onnx',\n",
        "                        'format': 'onnx',\n",
        "                        'dataset': 'ImageNet',\n",
        "                        'size': 'Large'\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            'audio_models': {\n",
        "                'description': 'Audio Processing Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'facebook/wav2vec2-base-960h',\n",
        "                        'filename': 'pytorch_model.bin',\n",
        "                        'local_name': 'wav2vec2_speech.bin',\n",
        "                        'format': 'pytorch',\n",
        "                        'dataset': 'LibriSpeech',\n",
        "                        'size': 'Large'\n",
        "                    }\n",
        "                ]\n",
        "            },\n",
        "            'tensorflow_models': {\n",
        "                'description': 'TensorFlow/Keras Models',\n",
        "                'models': [\n",
        "                    {\n",
        "                        'repo_id': 'tensorflow/mobilenet_v2_1.4_224',\n",
        "                        'filename': 'tf_model.h5',\n",
        "                        'local_name': 'mobilenet_v2.h5',\n",
        "                        'format': 'tensorflow',\n",
        "                        'dataset': 'ImageNet',\n",
        "                        'size': 'Medium'\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def list_available_models(self):\n",
        "        \"\"\"Display all available models\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"AVAILABLE MODELS FOR DOWNLOAD\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        total_models = 0\n",
        "        for category, info in self.model_catalog.items():\n",
        "            print(f\"\\n{info['description']}:\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "            for i, model in enumerate(info['models']):\n",
        "                total_models += 1\n",
        "                print(f\"  {total_models}. {model['local_name']}\")\n",
        "                print(f\"     Format: {model['format'].upper()}\")\n",
        "                print(f\"     Dataset: {model['dataset']}\")\n",
        "                print(f\"     Size: {model['size']}\")\n",
        "                print(f\"     Repository: {model['repo_id']}\")\n",
        "                print()\n",
        "\n",
        "        return total_models\n",
        "\n",
        "    def download_model(self, repo_id, filename, local_name, format_type):\n",
        "        \"\"\"Download a specific model\"\"\"\n",
        "        local_path = self.download_dir / local_name\n",
        "\n",
        "        print(f\"[DOWNLOAD] Starting download of {local_name}...\")\n",
        "        print(f\"[INFO] Repository: {repo_id}\")\n",
        "        print(f\"[INFO] File: {filename}\")\n",
        "        print(f\"[INFO] Format: {format_type}\")\n",
        "\n",
        "        try:\n",
        "            # Download the file\n",
        "            downloaded_path = hf_hub_download(\n",
        "                repo_id=repo_id,\n",
        "                filename=filename,\n",
        "                local_dir=str(self.download_dir),\n",
        "                local_dir_use_symlinks=False\n",
        "            )\n",
        "\n",
        "            # Rename to our preferred name\n",
        "            if Path(downloaded_path) != local_path:\n",
        "                Path(downloaded_path).rename(local_path)\n",
        "\n",
        "            # Get file size\n",
        "            file_size = local_path.stat().st_size / (1024 * 1024)  # MB\n",
        "\n",
        "            print(f\"[SUCCESS] Downloaded: {local_name} ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to download {local_name}: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def download_custom_onnx_models(self):\n",
        "        \"\"\"Download some basic ONNX models manually\"\"\"\n",
        "        print(\"[INFO] Downloading basic ONNX models...\")\n",
        "\n",
        "        # MNIST model from ONNX model zoo\n",
        "        onnx_models = [\n",
        "            {\n",
        "                'url': 'https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-8.onnx',\n",
        "                'filename': 'mnist_handwritten_digits.onnx'\n",
        "            },\n",
        "            {\n",
        "                'url': 'https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-1.onnx',\n",
        "                'filename': 'mnist_simple.onnx'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        for model in onnx_models:\n",
        "            try:\n",
        "                print(f\"[DOWNLOAD] {model['filename']}...\")\n",
        "                response = requests.get(model['url'], stream=True)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                local_path = self.download_dir / model['filename']\n",
        "                with open(local_path, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                file_size = local_path.stat().st_size / (1024 * 1024)\n",
        "                print(f\"[SUCCESS] Downloaded: {model['filename']} ({file_size:.2f} MB)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ERROR] Failed to download {model['filename']}: {e}\")\n",
        "\n",
        "    def create_sample_sklearn_models(self):\n",
        "        \"\"\"Create sample scikit-learn models\"\"\"\n",
        "        print(\"[INFO] Creating sample scikit-learn models...\")\n",
        "\n",
        "        try:\n",
        "            from sklearn.ensemble import RandomForestClassifier\n",
        "            from sklearn.linear_model import LogisticRegression\n",
        "            from sklearn.svm import SVC\n",
        "            from sklearn.datasets import make_classification\n",
        "            import pickle\n",
        "\n",
        "            # Generate sample data\n",
        "            X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "\n",
        "            # Create different model types\n",
        "            models = [\n",
        "                ('random_forest_classifier.pkl', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "                ('logistic_regression.pkl', LogisticRegression(random_state=42)),\n",
        "                ('svm_classifier.pkl', SVC(kernel='rbf', random_state=42))\n",
        "            ]\n",
        "\n",
        "            for filename, model in models:\n",
        "                # Train the model\n",
        "                model.fit(X, y)\n",
        "\n",
        "                # Save to pickle\n",
        "                model_path = self.download_dir / filename\n",
        "                with open(model_path, 'wb') as f:\n",
        "                    pickle.dump(model, f)\n",
        "\n",
        "                file_size = model_path.stat().st_size / 1024  # KB\n",
        "                print(f\"[SUCCESS] Created: {filename} ({file_size:.2f} KB)\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"[WARNING] scikit-learn not available, skipping sklearn models\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to create sklearn models: {e}\")\n",
        "\n",
        "    def download_interactive(self):\n",
        "        \"\"\"Interactive download interface\"\"\"\n",
        "        total_models = self.list_available_models()\n",
        "\n",
        "        print(\"=\"*80)\n",
        "        print(\"DOWNLOAD OPTIONS\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"1. Download ALL models (may take a while)\")\n",
        "        print(\"2. Download by category\")\n",
        "        print(\"3. Download specific models\")\n",
        "        print(\"4. Download basic ONNX models only\")\n",
        "        print(\"5. Create sample sklearn models only\")\n",
        "        print(\"6. Quick start pack (small models)\")\n",
        "\n",
        "        try:\n",
        "            choice = input(\"\\nSelect option (1-6): \").strip()\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nDownload cancelled.\")\n",
        "            return\n",
        "\n",
        "        if choice == '1':\n",
        "            self.download_all_models()\n",
        "        elif choice == '2':\n",
        "            self.download_by_category()\n",
        "        elif choice == '3':\n",
        "            self.download_specific_models()\n",
        "        elif choice == '4':\n",
        "            self.download_custom_onnx_models()\n",
        "        elif choice == '5':\n",
        "            self.create_sample_sklearn_models()\n",
        "        elif choice == '6':\n",
        "            self.download_quick_start()\n",
        "        else:\n",
        "            print(\"Invalid choice. Downloading quick start pack...\")\n",
        "            self.download_quick_start()\n",
        "\n",
        "    def download_quick_start(self):\n",
        "        \"\"\"Download a small set of models for quick testing\"\"\"\n",
        "        print(\"[INFO] Downloading quick start pack...\")\n",
        "\n",
        "        # Download basic ONNX models\n",
        "        self.download_custom_onnx_models()\n",
        "\n",
        "        # Create sklearn models\n",
        "        self.create_sample_sklearn_models()\n",
        "\n",
        "        # Try to download one small PyTorch model\n",
        "        try:\n",
        "            self.download_model(\n",
        "                'distilbert-base-uncased-finetuned-sst-2-english',\n",
        "                'pytorch_model.bin',\n",
        "                'distilbert_sentiment.bin',\n",
        "                'pytorch'\n",
        "            )\n",
        "        except:\n",
        "            print(\"[WARNING] Could not download PyTorch model\")\n",
        "\n",
        "        print(\"\\n[SUCCESS] Quick start pack downloaded!\")\n",
        "\n",
        "    def download_all_models(self):\n",
        "        \"\"\"Download all models in catalog\"\"\"\n",
        "        print(\"[INFO] Downloading ALL models... This may take a while!\")\n",
        "\n",
        "        success_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for category, info in self.model_catalog.items():\n",
        "            print(f\"\\n[CATEGORY] {info['description']}\")\n",
        "            for model in info['models']:\n",
        "                total_count += 1\n",
        "                if self.download_model(\n",
        "                    model['repo_id'],\n",
        "                    model['filename'],\n",
        "                    model['local_name'],\n",
        "                    model['format']\n",
        "                ):\n",
        "                    success_count += 1\n",
        "\n",
        "        # Also download custom models\n",
        "        self.download_custom_onnx_models()\n",
        "        self.create_sample_sklearn_models()\n",
        "\n",
        "        print(f\"\\n[COMPLETE] Downloaded {success_count}/{total_count} models from catalog\")\n",
        "\n",
        "    def download_by_category(self):\n",
        "        \"\"\"Download models by category\"\"\"\n",
        "        print(\"\\nAvailable Categories:\")\n",
        "        categories = list(self.model_catalog.keys())\n",
        "\n",
        "        for i, (category, info) in enumerate(self.model_catalog.items()):\n",
        "            print(f\"  {i+1}. {info['description']} ({len(info['models'])} models)\")\n",
        "\n",
        "        try:\n",
        "            choice = int(input(f\"\\nSelect category (1-{len(categories)}): \")) - 1\n",
        "            selected_category = categories[choice]\n",
        "\n",
        "            print(f\"\\n[INFO] Downloading {self.model_catalog[selected_category]['description']}...\")\n",
        "\n",
        "            for model in self.model_catalog[selected_category]['models']:\n",
        "                self.download_model(\n",
        "                    model['repo_id'],\n",
        "                    model['filename'],\n",
        "                    model['local_name'],\n",
        "                    model['format']\n",
        "                )\n",
        "\n",
        "        except (ValueError, IndexError):\n",
        "            print(\"Invalid selection.\")\n",
        "\n",
        "    def show_download_summary(self):\n",
        "        \"\"\"Show what was downloaded\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"DOWNLOAD SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # List all model files in download directory\n",
        "        extensions = ['.onnx', '.h5', '.hdf5', '.bin', '.pkl', '.pt', '.pth']\n",
        "        found_models = []\n",
        "\n",
        "        for ext in extensions:\n",
        "            found_models.extend(list(self.download_dir.glob(f\"*{ext}\")))\n",
        "\n",
        "        if found_models:\n",
        "            print(f\"Found {len(found_models)} model files in {self.download_dir}:\")\n",
        "            print()\n",
        "\n",
        "            total_size = 0\n",
        "            for model_file in found_models:\n",
        "                size_mb = model_file.stat().st_size / (1024 * 1024)\n",
        "                total_size += size_mb\n",
        "                print(f\"  {model_file.name:<40} {size_mb:>8.2f} MB\")\n",
        "\n",
        "            print(f\"\\nTotal size: {total_size:.2f} MB\")\n",
        "            print(f\"\\nYou can now use the reverse engineering script to analyze these models!\")\n",
        "        else:\n",
        "            print(\"No model files found. Try downloading some models first.\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function\"\"\"\n",
        "    print(\"HUGGING FACE MODEL DOWNLOADER\")\n",
        "    print(\"Downloads various ML models for reverse engineering analysis\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Install requirements\n",
        "    install_requirements()\n",
        "\n",
        "    # Create downloader\n",
        "    downloader = HuggingFaceModelDownloader()\n",
        "\n",
        "    # Check if models already exist\n",
        "    existing_models = list(Path(\"/content\").glob(\"*.onnx\")) + list(Path(\"/content\").glob(\"*.h5\")) + list(Path(\"/content\").glob(\"*.pkl\"))\n",
        "\n",
        "    if existing_models:\n",
        "        print(f\"\\n[INFO] Found {len(existing_models)} existing model files:\")\n",
        "        for model in existing_models[:5]:  # Show first 5\n",
        "            print(f\"  - {model.name}\")\n",
        "        if len(existing_models) > 5:\n",
        "            print(f\"  ... and {len(existing_models) - 5} more\")\n",
        "\n",
        "        overwrite = input(\"\\nDownload additional models? (y/n): \").lower().strip()\n",
        "        if overwrite != 'y':\n",
        "            print(\"Exiting...\")\n",
        "            return\n",
        "\n",
        "    # Run interactive download\n",
        "    try:\n",
        "        downloader.download_interactive()\n",
        "\n",
        "        # Show summary\n",
        "        downloader.show_download_summary()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"NEXT STEPS:\")\n",
        "        print(\"1. Run the model analysis script\")\n",
        "        print(\"2. Select a model to analyze\")\n",
        "        print(\"3. View the reverse engineering results!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\\nDownload interrupted by user.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during download: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "12fecdb6d4694f9c899c61190f6ee35e",
            "b1f225c70604464ba2051a75fc7efbf2",
            "f187d232e3e24e76a0882b9242d45ffd",
            "6c0a04cd1b7941acb1f76955a2e472b7",
            "0f896be730a8490dab7fe34796726d7b",
            "b3c2bcaa655a4b62aa2f1dc984a31fd3",
            "c30233db24f4459ca6056ab67f9d22b5",
            "90442643e73a4b58b9a9f70fe411d128",
            "52d47ed18760446086a182b07ad2493f",
            "8730ff89118a4cdfaac7acf6c9cc9d99",
            "d519d9e127434a4c8472507ab149d756"
          ]
        },
        "id": "bM2oS3UdTlFB",
        "outputId": "7e8875d5-f096-42dd-90dd-dbaee6aa6eb2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUGGING FACE MODEL DOWNLOADER\n",
            "Downloads various ML models for reverse engineering analysis\n",
            "============================================================\n",
            "[OK] huggingface_hub already installed\n",
            "[OK] requests already installed\n",
            "\n",
            "[INFO] Found 2 existing model files:\n",
            "  - mnist-8.onnx\n",
            "  - mnist-8-enhanced.onnx\n",
            "\n",
            "Download additional models? (y/n): y\n",
            "================================================================================\n",
            "AVAILABLE MODELS FOR DOWNLOAD\n",
            "================================================================================\n",
            "\n",
            "Image Classification Models:\n",
            "--------------------------------------------------\n",
            "  1. resnet50_imagenet.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: ImageNet\n",
            "     Size: Medium\n",
            "     Repository: microsoft/resnet-50\n",
            "\n",
            "  2. vit_base_imagenet.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: ImageNet\n",
            "     Size: Large\n",
            "     Repository: google/vit-base-patch16-224\n",
            "\n",
            "\n",
            "Text Classification Models:\n",
            "--------------------------------------------------\n",
            "  3. twitter_sentiment.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: Twitter\n",
            "     Size: Medium\n",
            "     Repository: cardiffnlp/twitter-roberta-base-sentiment-latest\n",
            "\n",
            "  4. distilbert_sst2.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: SST-2\n",
            "     Size: Small\n",
            "     Repository: distilbert-base-uncased-finetuned-sst-2-english\n",
            "\n",
            "\n",
            "Object Detection Models:\n",
            "--------------------------------------------------\n",
            "  5. table_detection.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: Custom\n",
            "     Size: Medium\n",
            "     Repository: microsoft/table-transformer-detection\n",
            "\n",
            "\n",
            "ONNX Format Models:\n",
            "--------------------------------------------------\n",
            "  6. mnist_digit_classifier.onnx\n",
            "     Format: ONNX\n",
            "     Dataset: MNIST\n",
            "     Size: Small\n",
            "     Repository: onnx/models\n",
            "\n",
            "  7. resnet50_imagenet.onnx\n",
            "     Format: ONNX\n",
            "     Dataset: ImageNet\n",
            "     Size: Large\n",
            "     Repository: onnx/models\n",
            "\n",
            "\n",
            "Audio Processing Models:\n",
            "--------------------------------------------------\n",
            "  8. wav2vec2_speech.bin\n",
            "     Format: PYTORCH\n",
            "     Dataset: LibriSpeech\n",
            "     Size: Large\n",
            "     Repository: facebook/wav2vec2-base-960h\n",
            "\n",
            "\n",
            "TensorFlow/Keras Models:\n",
            "--------------------------------------------------\n",
            "  9. mobilenet_v2.h5\n",
            "     Format: TENSORFLOW\n",
            "     Dataset: ImageNet\n",
            "     Size: Medium\n",
            "     Repository: tensorflow/mobilenet_v2_1.4_224\n",
            "\n",
            "================================================================================\n",
            "DOWNLOAD OPTIONS\n",
            "================================================================================\n",
            "1. Download ALL models (may take a while)\n",
            "2. Download by category\n",
            "3. Download specific models\n",
            "4. Download basic ONNX models only\n",
            "5. Create sample sklearn models only\n",
            "6. Quick start pack (small models)\n",
            "\n",
            "Select option (1-6): 6\n",
            "[INFO] Downloading quick start pack...\n",
            "[INFO] Downloading basic ONNX models...\n",
            "[DOWNLOAD] mnist_handwritten_digits.onnx...\n",
            "[ERROR] Failed to download mnist_handwritten_digits.onnx: 404 Client Error: Not Found for url: https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-8.onnx\n",
            "[DOWNLOAD] mnist_simple.onnx...\n",
            "[ERROR] Failed to download mnist_simple.onnx: 404 Client Error: Not Found for url: https://github.com/onnx/models/raw/main/vision/classification/mnist/model/mnist-1.onnx\n",
            "[INFO] Creating sample scikit-learn models...\n",
            "[SUCCESS] Created: random_forest_classifier.pkl (1294.45 KB)\n",
            "[SUCCESS] Created: logistic_regression.pkl (0.86 KB)\n",
            "[SUCCESS] Created: svm_classifier.pkl (84.18 KB)\n",
            "[DOWNLOAD] Starting download of distilbert_sentiment.bin...\n",
            "[INFO] Repository: distilbert-base-uncased-finetuned-sst-2-english\n",
            "[INFO] File: pytorch_model.bin\n",
            "[INFO] Format: pytorch\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12fecdb6d4694f9c899c61190f6ee35e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SUCCESS] Downloaded: distilbert_sentiment.bin (255.44 MB)\n",
            "\n",
            "[SUCCESS] Quick start pack downloaded!\n",
            "\n",
            "================================================================================\n",
            "DOWNLOAD SUMMARY\n",
            "================================================================================\n",
            "Found 8 model files in /content:\n",
            "\n",
            "  mnist-8.onnx                                 0.03 MB\n",
            "  mnist-8-enhanced.onnx                        1.65 MB\n",
            "  distilbert_sentiment.bin                   255.44 MB\n",
            "  vit_base_imagenet.bin                      330.31 MB\n",
            "  resnet50_imagenet.bin                       97.82 MB\n",
            "  logistic_regression.pkl                      0.00 MB\n",
            "  svm_classifier.pkl                           0.08 MB\n",
            "  random_forest_classifier.pkl                 1.26 MB\n",
            "\n",
            "Total size: 686.58 MB\n",
            "\n",
            "You can now use the reverse engineering script to analyze these models!\n",
            "\n",
            "============================================================\n",
            "NEXT STEPS:\n",
            "1. Run the model analysis script\n",
            "2. Select a model to analyze\n",
            "3. View the reverse engineering results!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install psutil"
      ],
      "metadata": {
        "id": "8sORDdC7acuZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cweM0lY3N_sU",
        "outputId": "ad8c985e-fb92-4be4-e3a2-c9dc44bf901a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPREHENSIVE AI MODEL ANALYZER\n",
            "Advanced analysis for AI developers\n",
            "================================================================================\n",
            "System Info:\n",
            "  Python Version:    3.12.11\n",
            "  Available RAM:     12.7 GB\n",
            "  Available Disk:    185.4 GB\n",
            "  GPU Available:     Unknown\n",
            "\n",
            "Scanning /content for AI models...\n",
            "  Found 2 .onnx files\n",
            "  Found 3 .bin files\n",
            "  Found 3 .pkl files\n",
            "\n",
            "Found 8 model file(s) (sorted by size):\n",
            "   1. vit_base_imagenet.bin                      330.31 MB  (  0 days old)\n",
            "   2. distilbert_sentiment.bin                   255.44 MB  (  0 days old)\n",
            "   3. resnet50_imagenet.bin                       97.82 MB  (  0 days old)\n",
            "   4. mnist-8-enhanced.onnx                        1.65 MB  (  0 days old)\n",
            "   5. random_forest_classifier.pkl                 1.26 MB  (  0 days old)\n",
            "   6. svm_classifier.pkl                           0.08 MB  (  0 days old)\n",
            "   7. mnist-8.onnx                                 0.03 MB  (  0 days old)\n",
            "   8. logistic_regression.pkl                      0.00 MB  (  0 days old)\n",
            "\n",
            "Total model storage: 686.58 MB\n",
            "\n",
            "Analysis options:\n",
            "  0. Analyze ALL models\n",
            "  1. vit_base_imagenet.bin\n",
            "  2. distilbert_sentiment.bin\n",
            "  3. resnet50_imagenet.bin\n",
            "  4. mnist-8-enhanced.onnx\n",
            "  5. random_forest_classifier.pkl\n",
            "  6. svm_classifier.pkl\n",
            "  7. mnist-8.onnx\n",
            "  8. logistic_regression.pkl\n",
            "\n",
            "Select option (0-8): 7\n",
            "\n",
            "Starting comprehensive analysis of: mnist-8.onnx\n",
            "================================================================================\n",
            "================================================================================\n",
            "COMPREHENSIVE AI MODEL ANALYSIS\n",
            "================================================================================\n",
            "File Name:           mnist-8.onnx\n",
            "File Size:           0.03 MB\n",
            "File Extension:      .onnx\n",
            "File Type:           ONNX Model\n",
            "Modified:            2025-08-28 01:11:45\n",
            "Memory Footprint:    ~0.03 MB (estimated in RAM)\n",
            "--------------------------------------------------------------------------------\n",
            "ONNX MODEL ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "Producer:            CNTK\n",
            "Producer Version:    2.5.1\n",
            "ONNX Version:        1\n",
            "Training Framework:  CNTK 2.5.1\n",
            "Graph Name:          CNTKGraph\n",
            "Total Parameters:    5,998\n",
            "Weight Tensors:      8\n",
            "Parameters Memory:   0.02 MB\n",
            "Model Overhead:      0.00 MB\n",
            "Total Operations:    12\n",
            "Compute-heavy Ops:   3\n",
            "\n",
            "Operation Breakdown:\n",
            "  Add                    3 ( 25.0%)\n",
            "  Conv                   2 ( 16.7%)\n",
            "  MatMul                 1 (  8.3%)\n",
            "  MaxPool                2 ( 16.7%)\n",
            "  Relu                   2 ( 16.7%)\n",
            "  Reshape                2 ( 16.7%)\n",
            "\n",
            "INPUT/OUTPUT ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "Inputs (9):\n",
            "  1. Input3\n",
            "     Shape: [1, 1, 28, 28]\n",
            "     Type:  float32\n",
            "     Size:  3.06 KB\n",
            "     Preprocessing: Normalize to [0,1], grayscale, 28x28 resize\n",
            "  2. Parameter5\n",
            "     Shape: [8, 1, 5, 5]\n",
            "     Type:  float32\n",
            "     Size:  0.78 KB\n",
            "  3. Parameter6\n",
            "     Shape: [8, 1, 1]\n",
            "     Type:  float32\n",
            "     Size:  0.03 KB\n",
            "  4. Parameter87\n",
            "     Shape: [16, 8, 5, 5]\n",
            "     Type:  float32\n",
            "     Size:  12.50 KB\n",
            "  5. Parameter88\n",
            "     Shape: [16, 1, 1]\n",
            "     Type:  float32\n",
            "     Size:  0.06 KB\n",
            "  6. Pooling160_Output_0_reshape0_shape\n",
            "     Shape: [2]\n",
            "     Type:  int64\n",
            "     Size:  0.00 KB\n",
            "  7. Parameter193\n",
            "     Shape: [16, 4, 4, 10]\n",
            "     Type:  float32\n",
            "     Size:  10.00 KB\n",
            "  8. Parameter193_reshape1_shape\n",
            "     Shape: [2]\n",
            "     Type:  int64\n",
            "     Size:  0.00 KB\n",
            "  9. Parameter194\n",
            "     Shape: [1, 10]\n",
            "     Type:  float32\n",
            "     Size:  0.04 KB\n",
            "\n",
            "Outputs (1):\n",
            "  1. Plus214_Output_0\n",
            "     Shape: [1, 10]\n",
            "     Type:  Classification probabilities (10 classes)\n",
            "\n",
            "PERFORMANCE ESTIMATES\n",
            "--------------------------------------------------------------------------------\n",
            "Estimated FLOPS:     0\n",
            "Inference Speed:     ~1000000.0 FPS (estimated)\n",
            "\n",
            "HARDWARE REQUIREMENTS\n",
            "--------------------------------------------------------------------------------\n",
            "Minimum Ram          4 GB\n",
            "Recommended Ram      8 GB\n",
            "Gpu Memory           2 GB\n",
            "Cpu Cores            2\n",
            "Storage              0 MB\n",
            "\n",
            "DEPLOYMENT INFORMATION\n",
            "--------------------------------------------------------------------------------\n",
            "Quantization:        Optional\n",
            "Compatibility:\n",
            "  CPU Inference:     Supported\n",
            "  GPU Inference:     Supported (CUDA/OpenCL)\n",
            "  Mobile (iOS):      Supported (Core ML conversion)\n",
            "  Mobile (Android):  Supported (TensorFlow Lite)\n",
            "  Web Browser:       Supported (ONNX.js)\n",
            "  Edge Devices:      Supported (optimized versions)\n",
            "\n",
            "Optimization Options:\n",
            "  - Convert to TensorRT for NVIDIA GPUs\n",
            "  - Use OpenVINO for Intel hardware\n",
            "  - Apply dynamic quantization\n",
            "  - Batch processing for throughput\n",
            "  - Model pruning for size reduction\n",
            "\n",
            "SECURITY & ROBUSTNESS ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "Potential Risks:     Low (based on analysis)\n",
            "\n",
            "Security Recommendations:\n",
            "  ✓ Monitor model outputs for unexpected behavior\n",
            "  ✓ Implement confidence thresholds\n",
            "  ✓ Log all inference requests for audit trail\n",
            "\n",
            "DATASET & USE CASE ANALYSIS\n",
            "--------------------------------------------------------------------------------\n",
            "Dataset              MNIST Handwritten Digits\n",
            "Confidence           High\n",
            "Task                 Digit Recognition\n",
            "Business Use         Document digitization, form processing\n",
            "Data Source          28x28 grayscale images\n",
            "Accuracy Expectation 99%+\n",
            "Licensing            Public domain\n",
            "\n",
            "COMPLIANCE CONSIDERATIONS\n",
            "----------------------------------------\n",
            "GDPR Compliance:     Depends on training data\n",
            "Bias Assessment:     Recommended before deployment\n",
            "Fairness Testing:    Required for production use\n",
            "Data Lineage:        Document training data sources\n",
            "\n",
            "DEPLOYMENT READINESS CHECKLIST\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Model Validation:\n",
            "  ☐ Test model loading and inference\n",
            "  ☐ Validate input/output shapes\n",
            "  ☐ Check numerical stability\n",
            "  ☐ Verify expected performance\n",
            "\n",
            "Infrastructure:\n",
            "  ☐ Set up monitoring and logging\n",
            "  ☐ Configure auto-scaling\n",
            "  ☐ Test backup and recovery\n",
            "  ☐ Set up health checks\n",
            "\n",
            "Security:\n",
            "  ☐ Input validation implementation\n",
            "  ☐ Rate limiting configuration\n",
            "  ☐ Authentication setup\n",
            "  ☐ Audit logging enabled\n",
            "\n",
            "Performance:\n",
            "  ☐ Benchmark inference speed\n",
            "  ☐ Memory usage profiling\n",
            "  ☐ Load testing completed\n",
            "  ☐ Optimization applied (if needed)\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS COMPLETE - Ready for production deployment!\n",
            "================================================================================\n",
            "\n",
            "NEXT STEPS FOR AI DEVELOPERS:\n",
            "1. Review security recommendations above\n",
            "2. Test model with sample inputs\n",
            "3. Benchmark performance on target hardware\n",
            "4. Implement monitoring and logging\n",
            "5. Plan model versioning strategy\n",
            "6. Document API interfaces and data formats\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import time\n",
        "import psutil\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def analyze_model_file(model_path):\n",
        "    \"\"\"Enhanced model file analyzer with AI developer insights\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE AI MODEL ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Basic file info\n",
        "    file_path = Path(model_path)\n",
        "    file_stat = file_path.stat()\n",
        "    file_size_mb = file_stat.st_size / (1024 * 1024)\n",
        "\n",
        "    print(f\"File Name:           {file_path.name}\")\n",
        "    print(f\"File Size:           {file_size_mb:.2f} MB\")\n",
        "    print(f\"File Extension:      {file_path.suffix}\")\n",
        "    print(f\"File Type:           {file_path.suffix.upper()[1:]} Model\")\n",
        "    print(f\"Modified:            {datetime.datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Memory Footprint:    ~{file_size_mb * 1.2:.2f} MB (estimated in RAM)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Try to load and analyze based on extension\n",
        "    ext = file_path.suffix.lower()\n",
        "\n",
        "    try:\n",
        "        if ext == '.onnx':\n",
        "            analyze_onnx_model(model_path)\n",
        "        elif ext in ['.h5', '.hdf5']:\n",
        "            analyze_keras_model(model_path)\n",
        "        elif ext in ['.pt', '.pth', '.bin']:\n",
        "            analyze_pytorch_model(model_path)\n",
        "        elif ext in ['.pkl', '.joblib']:\n",
        "            analyze_sklearn_model(model_path)\n",
        "        else:\n",
        "            print(f\"Model type {ext} not yet supported\")\n",
        "            print(\"Supported formats: .onnx, .h5, .hdf5, .pt, .pth, .bin, .pkl, .joblib\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing model: {e}\")\n",
        "        print(\"Basic file analysis completed.\")\n",
        "\n",
        "def calculate_flops_estimate(model_info):\n",
        "    \"\"\"Estimate FLOPS based on model architecture\"\"\"\n",
        "    flops = 0\n",
        "\n",
        "    if 'layers' in model_info:\n",
        "        for layer in model_info['layers']:\n",
        "            layer_type = layer.get('type', '').lower()\n",
        "            params = layer.get('params', 0)\n",
        "\n",
        "            if 'conv' in layer_type:\n",
        "                # Rough estimate: 2 * params for conv layers\n",
        "                flops += params * 2\n",
        "            elif 'dense' in layer_type or 'linear' in layer_type:\n",
        "                # Dense layer: 2 * input_size * output_size\n",
        "                flops += params * 2\n",
        "            elif 'attention' in layer_type:\n",
        "                # Attention is computationally expensive\n",
        "                flops += params * 4\n",
        "\n",
        "    elif 'operations' in model_info:\n",
        "        # ONNX model estimation\n",
        "        op_flops = {\n",
        "            'Conv': 1000000,  # ~1M FLOPS per conv\n",
        "            'MatMul': 500000,  # ~500K FLOPS per matmul\n",
        "            'Add': 1000,\n",
        "            'Relu': 1000,\n",
        "            'Softmax': 10000\n",
        "        }\n",
        "\n",
        "        for op_type, count in model_info.get('operation_types', {}).items():\n",
        "            flops += op_flops.get(op_type, 1000) * count\n",
        "\n",
        "    return flops\n",
        "\n",
        "def estimate_hardware_requirements(model_size_mb, total_params, model_type):\n",
        "    \"\"\"Estimate hardware requirements\"\"\"\n",
        "    requirements = {\n",
        "        'minimum_ram': f\"{max(4, model_size_mb * 2):.0f} GB\",\n",
        "        'recommended_ram': f\"{max(8, model_size_mb * 4):.0f} GB\",\n",
        "        'gpu_memory': f\"{max(2, model_size_mb * 1.5):.0f} GB\",\n",
        "        'cpu_cores': 4 if total_params > 100000000 else 2,\n",
        "        'storage': f\"{model_size_mb * 1.2:.0f} MB\"\n",
        "    }\n",
        "\n",
        "    # Adjust based on model type\n",
        "    if model_type in ['transformer', 'bert', 'gpt']:\n",
        "        requirements['gpu_memory'] = f\"{max(6, model_size_mb * 2):.0f} GB\"\n",
        "        requirements['minimum_ram'] = f\"{max(16, model_size_mb * 3):.0f} GB\"\n",
        "\n",
        "    return requirements\n",
        "\n",
        "def analyze_security_risks(model_info, filename):\n",
        "    \"\"\"Analyze potential security and robustness issues\"\"\"\n",
        "    risks = []\n",
        "    recommendations = []\n",
        "\n",
        "    # Check for common vulnerabilities\n",
        "    if 'total_params' in model_info:\n",
        "        params = model_info['total_params']\n",
        "        if params > 1000000000:  # > 1B parameters\n",
        "            risks.append(\"Large model - potential for adversarial attacks\")\n",
        "            recommendations.append(\"Implement input validation and adversarial training\")\n",
        "\n",
        "    # Check model type risks\n",
        "    filename_lower = filename.lower()\n",
        "    if 'bert' in filename_lower or 'gpt' in filename_lower:\n",
        "        risks.append(\"Language model - risk of generating harmful content\")\n",
        "        recommendations.append(\"Implement content filtering and safety checks\")\n",
        "\n",
        "    if 'vision' in filename_lower or 'image' in filename_lower:\n",
        "        risks.append(\"Vision model - susceptible to image-based attacks\")\n",
        "        recommendations.append(\"Add input preprocessing and validation\")\n",
        "\n",
        "    # Generic recommendations\n",
        "    recommendations.extend([\n",
        "        \"Monitor model outputs for unexpected behavior\",\n",
        "        \"Implement confidence thresholds\",\n",
        "        \"Log all inference requests for audit trail\"\n",
        "    ])\n",
        "\n",
        "    return risks, recommendations\n",
        "\n",
        "def generate_deployment_checklist(model_info, filename):\n",
        "    \"\"\"Generate deployment readiness checklist\"\"\"\n",
        "    checklist = {\n",
        "        'model_validation': [\n",
        "            \"Test model loading and inference\",\n",
        "            \"Validate input/output shapes\",\n",
        "            \"Check numerical stability\",\n",
        "            \"Verify expected performance\"\n",
        "        ],\n",
        "        'infrastructure': [\n",
        "            \"Set up monitoring and logging\",\n",
        "            \"Configure auto-scaling\",\n",
        "            \"Test backup and recovery\",\n",
        "            \"Set up health checks\"\n",
        "        ],\n",
        "        'security': [\n",
        "            \"Input validation implementation\",\n",
        "            \"Rate limiting configuration\",\n",
        "            \"Authentication setup\",\n",
        "            \"Audit logging enabled\"\n",
        "        ],\n",
        "        'performance': [\n",
        "            \"Benchmark inference speed\",\n",
        "            \"Memory usage profiling\",\n",
        "            \"Load testing completed\",\n",
        "            \"Optimization applied (if needed)\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    return checklist\n",
        "\n",
        "def analyze_onnx_model(model_path):\n",
        "    \"\"\"Enhanced ONNX model analysis\"\"\"\n",
        "    print(\"ONNX MODEL ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    try:\n",
        "        import onnx\n",
        "        model = onnx.load(model_path)\n",
        "\n",
        "        # Basic model info\n",
        "        producer = getattr(model, 'producer_name', 'Unknown')\n",
        "        producer_version = getattr(model, 'producer_version', 'Unknown')\n",
        "        onnx_version = getattr(model, 'model_version', 'Unknown')\n",
        "\n",
        "        print(f\"Producer:            {producer}\")\n",
        "        print(f\"Producer Version:    {producer_version}\")\n",
        "        print(f\"ONNX Version:        {onnx_version}\")\n",
        "        print(f\"Training Framework:  {producer} {producer_version}\")\n",
        "\n",
        "        # Graph analysis\n",
        "        graph = model.graph\n",
        "        graph_name = getattr(graph, 'name', 'Unnamed')\n",
        "        print(f\"Graph Name:          {graph_name}\")\n",
        "\n",
        "        # Parameter analysis\n",
        "        total_params = 0\n",
        "        param_details = []\n",
        "        weight_tensors = 0\n",
        "\n",
        "        if hasattr(graph, 'initializer'):\n",
        "            for init in graph.initializer:\n",
        "                if hasattr(init, 'dims'):\n",
        "                    param_count = 1\n",
        "                    for dim in init.dims:\n",
        "                        param_count *= dim\n",
        "                    total_params += param_count\n",
        "                    weight_tensors += 1\n",
        "\n",
        "                    param_details.append({\n",
        "                        'name': init.name,\n",
        "                        'shape': list(init.dims),\n",
        "                        'count': param_count,\n",
        "                        'type': 'weight' if 'weight' in init.name.lower() else 'bias'\n",
        "                    })\n",
        "\n",
        "        print(f\"Total Parameters:    {total_params:,}\")\n",
        "        print(f\"Weight Tensors:      {weight_tensors}\")\n",
        "\n",
        "        # Memory analysis\n",
        "        model_size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
        "        params_memory_mb = (total_params * 4) / (1024 * 1024)  # Assuming float32\n",
        "        overhead_mb = model_size_mb - params_memory_mb\n",
        "\n",
        "        print(f\"Parameters Memory:   {params_memory_mb:.2f} MB\")\n",
        "        print(f\"Model Overhead:      {overhead_mb:.2f} MB\")\n",
        "\n",
        "        # Operations analysis\n",
        "        num_operations = len(graph.node) if hasattr(graph, 'node') else 0\n",
        "        print(f\"Total Operations:    {num_operations}\")\n",
        "\n",
        "        op_types = {}\n",
        "        computational_ops = 0\n",
        "\n",
        "        if hasattr(graph, 'node') and graph.node:\n",
        "            for node in graph.node:\n",
        "                op_type = getattr(node, 'op_type', 'Unknown')\n",
        "                op_types[op_type] = op_types.get(op_type, 0) + 1\n",
        "\n",
        "                # Count computationally intensive operations\n",
        "                if op_type in ['Conv', 'MatMul', 'Gemm', 'ConvTranspose']:\n",
        "                    computational_ops += 1\n",
        "\n",
        "            print(f\"Compute-heavy Ops:   {computational_ops}\")\n",
        "\n",
        "            print(\"\\nOperation Breakdown:\")\n",
        "            for op_type, count in sorted(op_types.items()):\n",
        "                percentage = (count / num_operations) * 100\n",
        "                print(f\"  {op_type:<20} {count:>3} ({percentage:>5.1f}%)\")\n",
        "\n",
        "        # I/O Analysis with detailed shapes\n",
        "        analyze_model_io(graph)\n",
        "\n",
        "        # Performance estimates\n",
        "        flops = calculate_flops_estimate({'operation_types': op_types})\n",
        "        print(f\"\\nPERFORMANCE ESTIMATES\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Estimated FLOPS:     {flops:,}\")\n",
        "        print(f\"Inference Speed:     ~{1000/max(flops/1e9, 0.001):.1f} FPS (estimated)\")\n",
        "\n",
        "        # Hardware requirements\n",
        "        requirements = estimate_hardware_requirements(model_size_mb, total_params, 'onnx')\n",
        "        print(f\"\\nHARDWARE REQUIREMENTS\")\n",
        "        print(\"-\" * 80)\n",
        "        for req, value in requirements.items():\n",
        "            print(f\"{req.replace('_', ' ').title():<20} {value}\")\n",
        "\n",
        "        # Deployment information\n",
        "        print_deployment_info(model_size_mb, total_params, op_types)\n",
        "\n",
        "        # Security analysis\n",
        "        risks, recommendations = analyze_security_risks(\n",
        "            {'total_params': total_params, 'operation_types': op_types},\n",
        "            model_path.name\n",
        "        )\n",
        "        print_security_analysis(risks, recommendations)\n",
        "\n",
        "        # Dataset detection with enhanced info\n",
        "        detect_dataset_comprehensive(model_path.name, op_types, graph)\n",
        "\n",
        "        # Generate deployment checklist\n",
        "        checklist = generate_deployment_checklist(\n",
        "            {'total_params': total_params},\n",
        "            model_path.name\n",
        "        )\n",
        "        print_deployment_checklist(checklist)\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"ONNX not installed. Install with: !pip install onnx\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading ONNX model: {e}\")\n",
        "\n",
        "def analyze_model_io(graph):\n",
        "    \"\"\"Analyze model inputs and outputs with detailed information\"\"\"\n",
        "    print(f\"\\nINPUT/OUTPUT ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Input analysis\n",
        "    if hasattr(graph, 'input') and graph.input:\n",
        "        print(f\"Inputs ({len(graph.input)}):\")\n",
        "        for i, inp in enumerate(graph.input):\n",
        "            input_name = getattr(inp, 'name', f'input_{i}')\n",
        "\n",
        "            shape_info = \"Unknown shape\"\n",
        "            data_type = \"Unknown type\"\n",
        "            estimated_size = \"Unknown size\"\n",
        "\n",
        "            if hasattr(inp, 'type') and hasattr(inp.type, 'tensor_type'):\n",
        "                tensor_type = inp.type.tensor_type\n",
        "\n",
        "                # Get data type\n",
        "                if hasattr(tensor_type, 'elem_type'):\n",
        "                    type_map = {1: 'float32', 2: 'uint8', 3: 'int8', 6: 'int32', 7: 'int64', 11: 'double'}\n",
        "                    data_type = type_map.get(tensor_type.elem_type, f'type_{tensor_type.elem_type}')\n",
        "\n",
        "                # Get shape\n",
        "                if hasattr(tensor_type, 'shape') and hasattr(tensor_type.shape, 'dim'):\n",
        "                    dims = []\n",
        "                    total_elements = 1\n",
        "                    for dim in tensor_type.shape.dim:\n",
        "                        if hasattr(dim, 'dim_value') and dim.dim_value > 0:\n",
        "                            dims.append(str(dim.dim_value))\n",
        "                            total_elements *= dim.dim_value\n",
        "                        elif hasattr(dim, 'dim_param'):\n",
        "                            dims.append(dim.dim_param)\n",
        "                            total_elements = -1  # Dynamic\n",
        "\n",
        "                    if dims:\n",
        "                        shape_info = f\"[{', '.join(dims)}]\"\n",
        "                        if total_elements > 0:\n",
        "                            bytes_per_element = 4 if 'float32' in data_type else 1\n",
        "                            estimated_size = f\"{(total_elements * bytes_per_element / 1024):.2f} KB\"\n",
        "\n",
        "            print(f\"  {i+1}. {input_name}\")\n",
        "            print(f\"     Shape: {shape_info}\")\n",
        "            print(f\"     Type:  {data_type}\")\n",
        "            print(f\"     Size:  {estimated_size}\")\n",
        "\n",
        "            # Suggest preprocessing based on shape\n",
        "            if '[1, 1, 28, 28]' in shape_info:\n",
        "                print(f\"     Preprocessing: Normalize to [0,1], grayscale, 28x28 resize\")\n",
        "            elif '[1, 3, 224, 224]' in shape_info:\n",
        "                print(f\"     Preprocessing: ImageNet normalization, RGB, 224x224 resize\")\n",
        "\n",
        "    # Output analysis\n",
        "    if hasattr(graph, 'output') and graph.output:\n",
        "        print(f\"\\nOutputs ({len(graph.output)}):\")\n",
        "        for i, out in enumerate(graph.output):\n",
        "            output_name = getattr(out, 'name', f'output_{i}')\n",
        "\n",
        "            shape_info = \"Unknown shape\"\n",
        "            interpretation = \"\"\n",
        "\n",
        "            if hasattr(out, 'type') and hasattr(out.type, 'tensor_type'):\n",
        "                tensor_type = out.type.tensor_type\n",
        "                if hasattr(tensor_type, 'shape') and hasattr(tensor_type.shape, 'dim'):\n",
        "                    dims = []\n",
        "                    for dim in tensor_type.shape.dim:\n",
        "                        if hasattr(dim, 'dim_value'):\n",
        "                            dims.append(str(dim.dim_value))\n",
        "                        elif hasattr(dim, 'dim_param'):\n",
        "                            dims.append(dim.dim_param)\n",
        "\n",
        "                    if dims:\n",
        "                        shape_info = f\"[{', '.join(dims)}]\"\n",
        "\n",
        "                        # Interpret output shape\n",
        "                        if len(dims) == 2 and dims[1] in ['10', '1000', '21']:\n",
        "                            classes = dims[1]\n",
        "                            interpretation = f\"Classification probabilities ({classes} classes)\"\n",
        "                        elif 'regression' in output_name.lower():\n",
        "                            interpretation = \"Regression output\"\n",
        "\n",
        "            print(f\"  {i+1}. {output_name}\")\n",
        "            print(f\"     Shape: {shape_info}\")\n",
        "            if interpretation:\n",
        "                print(f\"     Type:  {interpretation}\")\n",
        "\n",
        "def print_deployment_info(model_size_mb, total_params, op_types=None):\n",
        "    \"\"\"Print deployment-specific information\"\"\"\n",
        "    print(f\"\\nDEPLOYMENT INFORMATION\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Quantization potential\n",
        "    if total_params > 1000000:  # > 1M parameters\n",
        "        print(\"Quantization:        Recommended (INT8/FP16)\")\n",
        "        print(f\"Quantized Size:      ~{model_size_mb/2:.1f} MB (estimated)\")\n",
        "    else:\n",
        "        print(\"Quantization:        Optional\")\n",
        "\n",
        "    # Platform compatibility\n",
        "    print(\"Compatibility:\")\n",
        "    print(\"  CPU Inference:     Supported\")\n",
        "    print(\"  GPU Inference:     Supported (CUDA/OpenCL)\")\n",
        "    print(\"  Mobile (iOS):      Supported (Core ML conversion)\")\n",
        "    print(\"  Mobile (Android):  Supported (TensorFlow Lite)\")\n",
        "    print(\"  Web Browser:       Supported (ONNX.js)\")\n",
        "    print(\"  Edge Devices:      Supported (optimized versions)\")\n",
        "\n",
        "    # Optimization suggestions\n",
        "    print(\"\\nOptimization Options:\")\n",
        "    if op_types and 'Conv' in op_types:\n",
        "        print(\"  - Convert to TensorRT for NVIDIA GPUs\")\n",
        "        print(\"  - Use OpenVINO for Intel hardware\")\n",
        "    print(\"  - Apply dynamic quantization\")\n",
        "    print(\"  - Batch processing for throughput\")\n",
        "    print(\"  - Model pruning for size reduction\")\n",
        "\n",
        "def print_security_analysis(risks, recommendations):\n",
        "    \"\"\"Print security analysis\"\"\"\n",
        "    print(f\"\\nSECURITY & ROBUSTNESS ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    if risks:\n",
        "        print(\"Potential Risks:\")\n",
        "        for risk in risks:\n",
        "            print(f\"  ⚠ {risk}\")\n",
        "    else:\n",
        "        print(\"Potential Risks:     Low (based on analysis)\")\n",
        "\n",
        "    print(\"\\nSecurity Recommendations:\")\n",
        "    for rec in recommendations:\n",
        "        print(f\"  ✓ {rec}\")\n",
        "\n",
        "def detect_dataset_comprehensive(filename, operations, graph=None):\n",
        "    \"\"\"Comprehensive dataset detection with business context\"\"\"\n",
        "    print(f\"\\nDATASET & USE CASE ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    filename_lower = filename.lower()\n",
        "\n",
        "    # Enhanced detection with business context\n",
        "    dataset_info = {\n",
        "        'mnist': {\n",
        "            'dataset': 'MNIST Handwritten Digits',\n",
        "            'confidence': 'High',\n",
        "            'task': 'Digit Recognition',\n",
        "            'business_use': 'Document digitization, form processing',\n",
        "            'data_source': '28x28 grayscale images',\n",
        "            'accuracy_expectation': '99%+',\n",
        "            'licensing': 'Public domain'\n",
        "        },\n",
        "        'cifar': {\n",
        "            'dataset': 'CIFAR-10/100 Object Recognition',\n",
        "            'confidence': 'High',\n",
        "            'task': 'Object Classification',\n",
        "            'business_use': 'Image categorization, content moderation',\n",
        "            'data_source': '32x32 color images',\n",
        "            'accuracy_expectation': '95%+',\n",
        "            'licensing': 'Academic use'\n",
        "        },\n",
        "        'imagenet': {\n",
        "            'dataset': 'ImageNet Large Scale Visual Recognition',\n",
        "            'confidence': 'High',\n",
        "            'task': 'Object Classification/Detection',\n",
        "            'business_use': 'E-commerce, content tagging, surveillance',\n",
        "            'data_source': '224x224+ color images',\n",
        "            'accuracy_expectation': '80%+ top-5',\n",
        "            'licensing': 'Research use'\n",
        "        },\n",
        "        'bert': {\n",
        "            'dataset': 'Large Text Corpora (Books, Wikipedia)',\n",
        "            'confidence': 'High',\n",
        "            'task': 'Natural Language Understanding',\n",
        "            'business_use': 'Chatbots, document analysis, search',\n",
        "            'data_source': 'Text sequences',\n",
        "            'accuracy_expectation': 'Task-dependent',\n",
        "            'licensing': 'Apache 2.0'\n",
        "        },\n",
        "        'sentiment': {\n",
        "            'dataset': 'Sentiment Analysis Dataset',\n",
        "            'confidence': 'High',\n",
        "            'task': 'Sentiment Classification',\n",
        "            'business_use': 'Social media monitoring, review analysis',\n",
        "            'data_source': 'Text reviews/comments',\n",
        "            'accuracy_expectation': '85%+',\n",
        "            'licensing': 'Varies'\n",
        "        }\n",
        "    }\n",
        "\n",
        "    detected_info = None\n",
        "    for keyword, info in dataset_info.items():\n",
        "        if keyword in filename_lower:\n",
        "            detected_info = info\n",
        "            break\n",
        "\n",
        "    if detected_info:\n",
        "        for key, value in detected_info.items():\n",
        "            print(f\"{key.replace('_', ' ').title():<20} {value}\")\n",
        "    else:\n",
        "        # Fallback analysis\n",
        "        print(\"Dataset Type:        Unknown\")\n",
        "        print(\"Confidence:          Low\")\n",
        "        print(\"Business Use:        Requires domain analysis\")\n",
        "\n",
        "        # Try to infer from operations\n",
        "        if operations:\n",
        "            if 'Conv' in operations:\n",
        "                print(\"Likely Domain:       Computer Vision\")\n",
        "                print(\"Suggested Use:       Image processing applications\")\n",
        "\n",
        "    # Add compliance information\n",
        "    print(f\"\\nCOMPLIANCE CONSIDERATIONS\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"GDPR Compliance:     Depends on training data\")\n",
        "    print(\"Bias Assessment:     Recommended before deployment\")\n",
        "    print(\"Fairness Testing:    Required for production use\")\n",
        "    print(\"Data Lineage:        Document training data sources\")\n",
        "\n",
        "def print_deployment_checklist(checklist):\n",
        "    \"\"\"Print deployment readiness checklist\"\"\"\n",
        "    print(f\"\\nDEPLOYMENT READINESS CHECKLIST\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for category, items in checklist.items():\n",
        "        print(f\"\\n{category.replace('_', ' ').title()}:\")\n",
        "        for item in items:\n",
        "            print(f\"  ☐ {item}\")\n",
        "\n",
        "def analyze_keras_model(model_path):\n",
        "    \"\"\"Enhanced Keras model analysis with AI developer insights\"\"\"\n",
        "    print(\"TENSORFLOW/KERAS MODEL ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "\n",
        "        # Load model and measure loading time\n",
        "        start_time = time.time()\n",
        "        model = tf.keras.models.load_model(model_path)\n",
        "        load_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Model Type:          {type(model).__name__}\")\n",
        "        print(f\"Loading Time:        {load_time:.2f} seconds\")\n",
        "        print(f\"TensorFlow Version:  {tf.__version__}\")\n",
        "\n",
        "        # Parameter analysis\n",
        "        total_params = model.count_params()\n",
        "        trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
        "        non_trainable_params = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
        "\n",
        "        print(f\"Total Parameters:    {total_params:,}\")\n",
        "        print(f\"Trainable Params:    {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
        "        print(f\"Non-trainable:       {non_trainable_params:,} ({non_trainable_params/total_params*100:.1f}%)\")\n",
        "        print(f\"Total Layers:        {len(model.layers)}\")\n",
        "\n",
        "        # Memory analysis\n",
        "        model_size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
        "        estimated_inference_memory = model_size_mb * 2.5  # Rough estimate\n",
        "\n",
        "        print(f\"Model Size:          {model_size_mb:.2f} MB\")\n",
        "        print(f\"Inference Memory:    ~{estimated_inference_memory:.0f} MB (estimated)\")\n",
        "\n",
        "        # Architecture analysis\n",
        "        input_shape = getattr(model, 'input_shape', None)\n",
        "        output_shape = getattr(model, 'output_shape', None)\n",
        "\n",
        "        if input_shape:\n",
        "            print(f\"Input Shape:         {input_shape}\")\n",
        "            # Calculate input size\n",
        "            if isinstance(input_shape, tuple) and len(input_shape) > 1:\n",
        "                input_size = 1\n",
        "                for dim in input_shape[1:]:  # Skip batch dimension\n",
        "                    if dim:\n",
        "                        input_size *= dim\n",
        "                input_mb = (input_size * 4) / (1024 * 1024)  # float32\n",
        "                print(f\"Input Size:          {input_size:,} elements ({input_mb:.2f} MB)\")\n",
        "\n",
        "        if output_shape:\n",
        "            print(f\"Output Shape:        {output_shape}\")\n",
        "\n",
        "        # Layer analysis with performance insights\n",
        "        layer_types = {}\n",
        "        layer_details = []\n",
        "        total_layer_params = 0\n",
        "\n",
        "        print(f\"\\nLAYER ARCHITECTURE\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            layer_type = type(layer).__name__\n",
        "            layer_types[layer_type] = layer_types.get(layer_type, 0) + 1\n",
        "            layer_params = layer.count_params()\n",
        "            total_layer_params += layer_params\n",
        "\n",
        "            if i < 15:  # Show first 15 layers\n",
        "                layer_details.append({\n",
        "                    'index': i,\n",
        "                    'name': layer.name,\n",
        "                    'type': layer_type,\n",
        "                    'params': layer_params,\n",
        "                    'output_shape': str(getattr(layer, 'output_shape', 'Unknown')),\n",
        "                    'trainable': layer.trainable\n",
        "                })\n",
        "\n",
        "        # Print layer details\n",
        "        for layer in layer_details:\n",
        "            trainable_mark = \"✓\" if layer['trainable'] else \"✗\"\n",
        "            print(f\"  {layer['index']+1:2}. {layer['name']:<25} {layer['type']:<15} {layer['params']:>8,} {trainable_mark}\")\n",
        "\n",
        "        if len(model.layers) > 15:\n",
        "            print(f\"       ... and {len(model.layers) - 15} more layers\")\n",
        "\n",
        "        print(f\"\\nLayer Type Summary:\")\n",
        "        for layer_type, count in sorted(layer_types.items()):\n",
        "            print(f\"  {layer_type:<20} {count:>3}\")\n",
        "\n",
        "        # Performance estimation\n",
        "        flops = calculate_flops_estimate({'layers': layer_details})\n",
        "\n",
        "        print(f\"\\nPERFORMANCE ANALYSIS\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"Estimated FLOPS:     {flops:,}\")\n",
        "\n",
        "        # Try to do a simple benchmark\n",
        "        try:\n",
        "            if input_shape and len(input_shape) > 1:\n",
        "                # Create dummy input\n",
        "                dummy_input = tf.random.normal([1] + list(input_shape[1:]))\n",
        "\n",
        "                # Warmup\n",
        "                for _ in range(3):\n",
        "                    _ = model(dummy_input, training=False)\n",
        "\n",
        "                # Benchmark\n",
        "                start_time = time.time()\n",
        "                for _ in range(10):\n",
        "                    _ = model(dummy_input, training=False)\n",
        "                avg_time = (time.time() - start_time) / 10\n",
        "\n",
        "                print(f\"Inference Time:      {avg_time*1000:.2f} ms (average of 10)\")\n",
        "                print(f\"Throughput:          {1/avg_time:.1f} FPS\")\n",
        "        except:\n",
        "            print(\"Inference Time:      Could not benchmark\")\n",
        "\n",
        "        # Hardware requirements\n",
        "        requirements = estimate_hardware_requirements(model_size_mb, total_params, 'keras')\n",
        "        print(f\"\\nHARDWARE REQUIREMENTS\")\n",
        "        print(\"-\" * 80)\n",
        "        for req, value in requirements.items():\n",
        "            print(f\"{req.replace('_', ' ').title():<20} {value}\")\n",
        "\n",
        "        # Deployment information\n",
        "        print(f\"\\nDEPLOYMENT OPTIONS\")\n",
        "        print(\"-\" * 80)\n",
        "        print(\"TensorFlow Serving:  Ready\")\n",
        "        print(\"TensorFlow Lite:     Convertible\")\n",
        "        print(\"TensorFlow.js:       Convertible\")\n",
        "        print(\"ONNX Export:         Supported\")\n",
        "        print(\"SavedModel Format:   Native\")\n",
        "\n",
        "        # Model compilation info\n",
        "        try:\n",
        "            if hasattr(model, 'optimizer') and model.optimizer:\n",
        "                print(f\"\\nTRAINING CONFIGURATION\")\n",
        "                print(\"-\" * 80)\n",
        "                print(f\"Optimizer:           {type(model.optimizer).__name__}\")\n",
        "                try:\n",
        "                    lr = model.optimizer.learning_rate\n",
        "                    if hasattr(lr, 'numpy'):\n",
        "                        lr = lr.numpy()\n",
        "                    print(f\"Learning Rate:       {lr}\")\n",
        "                except:\n",
        "                    print(\"Learning Rate:       Not available\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Security analysis\n",
        "        risks, recommendations = analyze_security_risks(\n",
        "            {'total_params': total_params, 'layers': layer_details},\n",
        "            model_path.name\n",
        "        )\n",
        "        print_security_analysis(risks, recommendations)\n",
        "\n",
        "        # Dataset detection\n",
        "        input_shape_str = str(input_shape) if input_shape else \"\"\n",
        "        detect_dataset_comprehensive(model_path.name, layer_types, input_shape_str)\n",
        "\n",
        "        # Deployment checklist\n",
        "        checklist = generate_deployment_checklist(\n",
        "            {'total_params': total_params, 'layers': layer_details},\n",
        "            model_path.name\n",
        "        )\n",
        "        print_deployment_checklist(checklist)\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"TensorFlow not installed. Install with: !pip install tensorflow\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Keras model: {e}\")\n",
        "\n",
        "def analyze_pytorch_model(model_path):\n",
        "    \"\"\"Enhanced PyTorch model analysis\"\"\"\n",
        "    print(\"PYTORCH MODEL ANALYSIS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "\n",
        "        # Load model and measure time\n",
        "        start_time = time.time()\n",
        "        model = torch.load(model_path, map_location='cpu')\n",
        "        load_time = time.time() - start_time\n",
        "\n",
        "        print(f\"PyTorch Version:     {torch.__version__}\")\n",
        "        print(f\"Loading Time:        {load_time:.2f} seconds\")\n",
        "\n",
        "        if isinstance(model, dict):\n",
        "            analyze_pytorch_state_dict(model, model_path)\n",
        "        elif hasattr(model, 'parameters'):\n",
        "            analyze_pytorch_model_object(model, model_path)\n",
        "        else:\n",
        "            print(f\"Model Type:          {type(model).__name__}\")\n",
        "            print(f\"Model Content:       Unknown format\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"PyTorch not installed. Install with: !pip install torch\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading PyTorch model: {e}\")\n",
        "\n",
        "def analyze_pytorch_state_dict(model, model_path):\n",
        "    \"\"\"Analyze PyTorch state dictionary\"\"\"\n",
        "    print(\"Model Type:          State Dictionary\")\n",
        "\n",
        "    # Parameter analysis\n",
        "    total_params = 0\n",
        "    tensor_count = 0\n",
        "    param_details = []\n",
        "    layer_info = {}\n",
        "\n",
        "    for key, param in model.items():\n",
        "        if torch.is_tensor(param):\n",
        "            tensor_count += 1\n",
        "            param_count = param.numel()\n",
        "            total_params += param_count\n",
        "\n",
        "            # Extract layer name\n",
        "            layer_name = key.split('.')[0] if '.' in key else key\n",
        "            if layer_name not in layer_info:\n",
        "                layer_info[layer_name] = {'params': 0, 'tensors': 0}\n",
        "            layer_info[layer_name]['params'] += param_count\n",
        "            layer_info[layer_name]['tensors'] += 1\n",
        "\n",
        "            param_details.append({\n",
        "                'name': key,\n",
        "                'shape': list(param.shape),\n",
        "                'count': param_count,\n",
        "               'dtype': str(param.dtype),\n",
        "               'layer': layer_name\n",
        "           })\n",
        "\n",
        "    print(f\"Total Parameters:    {total_params:,}\")\n",
        "    print(f\"Parameter Tensors:   {tensor_count}\")\n",
        "    print(f\"Unique Layers:       {len(layer_info)}\")\n",
        "\n",
        "    # Memory analysis\n",
        "    model_size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
        "    param_memory_mb = (total_params * 4) / (1024 * 1024)  # Assuming float32\n",
        "\n",
        "    print(f\"Model Size:          {model_size_mb:.2f} MB\")\n",
        "    print(f\"Parameters Memory:   {param_memory_mb:.2f} MB\")\n",
        "    print(f\"Overhead:            {model_size_mb - param_memory_mb:.2f} MB\")\n",
        "\n",
        "    # Layer breakdown\n",
        "    print(f\"\\nLAYER BREAKDOWN\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, (layer_name, info) in enumerate(layer_info.items()):\n",
        "       if i < 10:  # Show first 10 layers\n",
        "           param_pct = (info['params'] / total_params) * 100\n",
        "           print(f\"  {layer_name:<30} {info['params']:>10,} params ({param_pct:>5.1f}%)\")\n",
        "\n",
        "    if len(layer_info) > 10:\n",
        "       print(f\"       ... and {len(layer_info) - 10} more layers\")\n",
        "\n",
        "    # Parameter details\n",
        "    print(f\"\\nPARAMETER DETAILS (First 10)\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, param in enumerate(param_details[:10]):\n",
        "       shape_str = str(param['shape'])\n",
        "       print(f\"  {i+1:2}. {param['name']:<35} {shape_str:<20} {param['count']:>8,}\")\n",
        "\n",
        "    if len(param_details) > 10:\n",
        "       print(f\"       ... and {len(param_details) - 10} more parameters\")\n",
        "\n",
        "    # Try to infer architecture type\n",
        "    architecture_hints = []\n",
        "    for key in model.keys():\n",
        "       if 'conv' in key.lower():\n",
        "           architecture_hints.append('Convolutional Neural Network')\n",
        "           break\n",
        "       elif 'transformer' in key.lower() or 'attention' in key.lower():\n",
        "           architecture_hints.append('Transformer Architecture')\n",
        "           break\n",
        "       elif 'lstm' in key.lower() or 'gru' in key.lower():\n",
        "           architecture_hints.append('Recurrent Neural Network')\n",
        "           break\n",
        "\n",
        "    if architecture_hints:\n",
        "       print(f\"\\nARCHITECTURE TYPE\")\n",
        "       print(\"-\" * 80)\n",
        "       for hint in architecture_hints:\n",
        "           print(f\"Detected:            {hint}\")\n",
        "\n",
        "    # Performance estimates\n",
        "    print(f\"\\nPERFORMANCE ESTIMATES\")\n",
        "    print(\"-\" * 80)\n",
        "    estimated_flops = total_params * 2  # Rough estimate\n",
        "    print(f\"Estimated FLOPS:     {estimated_flops:,}\")\n",
        "    print(f\"Memory Bandwidth:    ~{param_memory_mb * 2:.0f} MB/s required\")\n",
        "\n",
        "    # Hardware requirements\n",
        "    requirements = estimate_hardware_requirements(model_size_mb, total_params, 'pytorch')\n",
        "    print(f\"\\nHARDWARE REQUIREMENTS\")\n",
        "    print(\"-\" * 80)\n",
        "    for req, value in requirements.items():\n",
        "       print(f\"{req.replace('_', ' ').title():<20} {value}\")\n",
        "\n",
        "    # Deployment options\n",
        "    print(f\"\\nDEPLOYMENT OPTIONS\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"TorchServe:          Ready\")\n",
        "    print(\"ONNX Export:         torch.onnx.export()\")\n",
        "    print(\"TorchScript:         torch.jit.trace()\")\n",
        "    print(\"Mobile (iOS):        PyTorch Mobile\")\n",
        "    print(\"Mobile (Android):    PyTorch Mobile\")\n",
        "    print(\"C++ Deployment:      LibTorch\")\n",
        "\n",
        "    # Security analysis\n",
        "    risks, recommendations = analyze_security_risks(\n",
        "       {'total_params': total_params},\n",
        "       model_path.name\n",
        "     )\n",
        "    print_security_analysis(risks, recommendations)\n",
        "\n",
        "    # Dataset detection\n",
        "    detect_dataset_comprehensive(model_path.name, {})\n",
        "\n",
        "def analyze_pytorch_model_object(model, model_path):\n",
        "   \"\"\"Analyze PyTorch model object\"\"\"\n",
        "   print(f\"Model Type:          {type(model).__name__}\")\n",
        "\n",
        "   # Parameter analysis\n",
        "   total_params = sum(p.numel() for p in model.parameters())\n",
        "   trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "   non_trainable_params = total_params - trainable_params\n",
        "\n",
        "   print(f\"Total Parameters:    {total_params:,}\")\n",
        "   print(f\"Trainable Params:    {trainable_params:,} ({trainable_params/total_params*100:.1f}%)\")\n",
        "   print(f\"Non-trainable:       {non_trainable_params:,} ({non_trainable_params/total_params*100:.1f}%)\")\n",
        "\n",
        "   # Module analysis\n",
        "   modules = list(model.named_modules())\n",
        "   print(f\"Total Modules:       {len(modules) - 1}\")  # Exclude root\n",
        "\n",
        "   # Module breakdown\n",
        "   module_types = {}\n",
        "   module_details = []\n",
        "\n",
        "   for name, module in modules:\n",
        "       if name:  # Skip root module\n",
        "           module_type = type(module).__name__\n",
        "           module_types[module_type] = module_types.get(module_type, 0) + 1\n",
        "\n",
        "           # Calculate parameters for this module\n",
        "           module_params = sum(p.numel() for p in module.parameters())\n",
        "\n",
        "           module_details.append({\n",
        "               'name': name,\n",
        "               'type': module_type,\n",
        "               'parameters': module_params\n",
        "           })\n",
        "\n",
        "   print(f\"\\nMODULE TYPES\")\n",
        "   print(\"-\" * 80)\n",
        "   for module_type, count in sorted(module_types.items()):\n",
        "       print(f\"  {module_type:<25} {count:>3}\")\n",
        "\n",
        "   # Show module hierarchy (first 15)\n",
        "   print(f\"\\nMODULE HIERARCHY (First 15)\")\n",
        "   print(\"-\" * 80)\n",
        "   for i, module in enumerate(module_details[:15]):\n",
        "       print(f\"  {i+1:2}. {module['name']:<35} {module['type']:<15} {module['parameters']:>8,}\")\n",
        "\n",
        "   if len(module_details) > 15:\n",
        "       print(f\"       ... and {len(module_details) - 15} more modules\")\n",
        "\n",
        "   # Try to perform inference benchmark\n",
        "   try:\n",
        "       model.eval()\n",
        "       # Try to infer input shape from first layer\n",
        "       first_layer = next(model.children())\n",
        "       if hasattr(first_layer, 'in_features'):\n",
        "           # Linear layer\n",
        "           dummy_input = torch.randn(1, first_layer.in_features)\n",
        "       elif hasattr(first_layer, 'in_channels'):\n",
        "           # Conv layer - assume common image size\n",
        "           dummy_input = torch.randn(1, first_layer.in_channels, 224, 224)\n",
        "       else:\n",
        "           dummy_input = None\n",
        "\n",
        "       if dummy_input is not None:\n",
        "           print(f\"\\nPERFORMANCE BENCHMARK\")\n",
        "           print(\"-\" * 80)\n",
        "\n",
        "           # Warmup\n",
        "           with torch.no_grad():\n",
        "               for _ in range(3):\n",
        "                   try:\n",
        "                       _ = model(dummy_input)\n",
        "                   except:\n",
        "                       break\n",
        "               else:\n",
        "                   # Actual benchmark\n",
        "                   start_time = time.time()\n",
        "                   for _ in range(10):\n",
        "                       _ = model(dummy_input)\n",
        "                   avg_time = (time.time() - start_time) / 10\n",
        "\n",
        "                   print(f\"Inference Time:      {avg_time*1000:.2f} ms (average)\")\n",
        "                   print(f\"Throughput:          {1/avg_time:.1f} FPS\")\n",
        "                   print(f\"Input Shape:         {list(dummy_input.shape)}\")\n",
        "   except:\n",
        "       print(f\"\\nPERFORMANCE BENCHMARK\")\n",
        "       print(\"-\" * 80)\n",
        "       print(\"Benchmark:           Could not run (unknown input shape)\")\n",
        "\n",
        "def analyze_sklearn_model(model_path):\n",
        "   \"\"\"Enhanced scikit-learn model analysis\"\"\"\n",
        "   print(\"SCIKIT-LEARN MODEL ANALYSIS\")\n",
        "   print(\"-\" * 80)\n",
        "\n",
        "   try:\n",
        "       import pickle\n",
        "       import sklearn\n",
        "\n",
        "       with open(model_path, 'rb') as f:\n",
        "           model = pickle.load(f)\n",
        "\n",
        "       print(f\"Model Type:          {type(model).__name__}\")\n",
        "       print(f\"Module:              {model.__module__}\")\n",
        "       print(f\"Scikit-learn Ver:    {sklearn.__version__}\")\n",
        "\n",
        "       # Model properties analysis\n",
        "       properties = []\n",
        "\n",
        "       # Basic properties\n",
        "       if hasattr(model, 'n_features_in_'):\n",
        "           properties.append(f\"Input Features: {model.n_features_in_}\")\n",
        "       if hasattr(model, 'n_classes_'):\n",
        "           properties.append(f\"Output Classes: {model.n_classes_}\")\n",
        "       if hasattr(model, 'classes_'):\n",
        "           classes = getattr(model, 'classes_')\n",
        "           properties.append(f\"Class Labels: {list(classes)[:5]}{'...' if len(classes) > 5 else ''}\")\n",
        "\n",
        "       # Model-specific properties\n",
        "       if hasattr(model, 'n_estimators'):\n",
        "           properties.append(f\"Estimators: {model.n_estimators}\")\n",
        "       if hasattr(model, 'max_depth'):\n",
        "           properties.append(f\"Max Depth: {model.max_depth}\")\n",
        "       if hasattr(model, 'C'):\n",
        "           properties.append(f\"Regularization C: {model.C}\")\n",
        "       if hasattr(model, 'kernel'):\n",
        "           properties.append(f\"Kernel: {model.kernel}\")\n",
        "\n",
        "       print(f\"\\nMODEL PROPERTIES\")\n",
        "       print(\"-\" * 80)\n",
        "       for prop in properties:\n",
        "           print(f\"  {prop}\")\n",
        "\n",
        "       # Hyperparameters\n",
        "       try:\n",
        "           params = model.get_params()\n",
        "           print(f\"\\nHYPERPARAMETERS ({len(params)} total)\")\n",
        "           print(\"-\" * 80)\n",
        "\n",
        "           # Show most important parameters\n",
        "           important_params = ['n_estimators', 'max_depth', 'learning_rate', 'C', 'gamma',\n",
        "                             'kernel', 'random_state', 'max_features', 'min_samples_split']\n",
        "\n",
        "           shown_params = 0\n",
        "           for param_name in important_params:\n",
        "               if param_name in params and shown_params < 10:\n",
        "                   print(f\"  {param_name:<20} {params[param_name]}\")\n",
        "                   shown_params += 1\n",
        "\n",
        "           if len(params) > shown_params:\n",
        "               print(f\"  ... and {len(params) - shown_params} more parameters\")\n",
        "\n",
        "       except:\n",
        "           print(\"Hyperparameters:     Could not extract\")\n",
        "\n",
        "       # Model complexity analysis\n",
        "       model_complexity = analyze_sklearn_complexity(model)\n",
        "       print(f\"\\nCOMPLEXITY ANALYSIS\")\n",
        "       print(\"-\" * 80)\n",
        "       for metric, value in model_complexity.items():\n",
        "           print(f\"{metric:<20} {value}\")\n",
        "\n",
        "       # Performance characteristics\n",
        "       print(f\"\\nPERFORMANCE CHARACTERISTICS\")\n",
        "       print(\"-\" * 80)\n",
        "\n",
        "       model_type = type(model).__name__.lower()\n",
        "       if 'forest' in model_type or 'tree' in model_type:\n",
        "           print(\"Training Speed:      Fast to Medium\")\n",
        "           print(\"Inference Speed:     Fast\")\n",
        "           print(\"Memory Usage:        Medium\")\n",
        "           print(\"Interpretability:    High (tree-based)\")\n",
        "           print(\"Handles Missing:     Yes\")\n",
        "           print(\"Feature Importance:  Available\")\n",
        "       elif 'svm' in model_type:\n",
        "           print(\"Training Speed:      Slow\")\n",
        "           print(\"Inference Speed:     Medium\")\n",
        "           print(\"Memory Usage:        Medium to High\")\n",
        "           print(\"Interpretability:    Low\")\n",
        "           print(\"Handles Missing:     No\")\n",
        "           print(\"Kernel Trick:        Yes\")\n",
        "       elif 'linear' in model_type or 'logistic' in model_type:\n",
        "           print(\"Training Speed:      Fast\")\n",
        "           print(\"Inference Speed:     Very Fast\")\n",
        "           print(\"Memory Usage:        Low\")\n",
        "           print(\"Interpretability:    High (coefficients)\")\n",
        "           print(\"Handles Missing:     No\")\n",
        "           print(\"Regularization:      Available\")\n",
        "\n",
        "       # Business applications\n",
        "       print(f\"\\nBUSINESS APPLICATIONS\")\n",
        "       print(\"-\" * 80)\n",
        "\n",
        "       if 'classifier' in model_type:\n",
        "           print(\"Use Case:            Classification problems\")\n",
        "           print(\"Output:              Class probabilities/labels\")\n",
        "           print(\"Applications:        Fraud detection, spam filtering, diagnosis\")\n",
        "       elif 'regressor' in model_type or 'regression' in model_type:\n",
        "           print(\"Use Case:            Regression problems\")\n",
        "           print(\"Output:              Continuous values\")\n",
        "           print(\"Applications:        Price prediction, demand forecasting\")\n",
        "       elif 'cluster' in model_type:\n",
        "           print(\"Use Case:            Unsupervised clustering\")\n",
        "           print(\"Output:              Cluster assignments\")\n",
        "           print(\"Applications:        Customer segmentation, anomaly detection\")\n",
        "\n",
        "       # Deployment recommendations\n",
        "       print(f\"\\nDEPLOYMENT RECOMMENDATIONS\")\n",
        "       print(\"-\" * 80)\n",
        "       print(\"Serialization:       Pickle (current), Joblib (recommended)\")\n",
        "       print(\"Production:          Flask/FastAPI REST API\")\n",
        "       print(\"Scaling:             Stateless, easily parallelizable\")\n",
        "       print(\"Monitoring:          Track prediction distribution drift\")\n",
        "       print(\"Updates:             Retrain periodically with new data\")\n",
        "\n",
        "       # Feature importance if available\n",
        "       if hasattr(model, 'feature_importances_'):\n",
        "           importances = model.feature_importances_\n",
        "           print(f\"\\nFEATURE IMPORTANCE (Top 10)\")\n",
        "           print(\"-\" * 80)\n",
        "           top_features = sorted(enumerate(importances), key=lambda x: x[1], reverse=True)[:10]\n",
        "           for i, (feature_idx, importance) in enumerate(top_features):\n",
        "               print(f\"  Feature {feature_idx:<3} {importance:>8.4f}\")\n",
        "\n",
        "       # Model coefficients if available\n",
        "       elif hasattr(model, 'coef_'):\n",
        "           coef = model.coef_\n",
        "           print(f\"\\nMODEL COEFFICIENTS\")\n",
        "           print(\"-\" * 80)\n",
        "           if len(coef.shape) == 1:\n",
        "               print(f\"Coefficients Shape:  {coef.shape}\")\n",
        "               print(f\"Top Coefficients:    {coef[:5]} ...\")\n",
        "           else:\n",
        "               print(f\"Coefficients Shape:  {coef.shape}\")\n",
        "               print(f\"Classes x Features:  {coef.shape[0]} x {coef.shape[1]}\")\n",
        "\n",
        "       # Security and robustness\n",
        "       risks, recommendations = analyze_security_risks({'model_type': model_type}, model_path.name)\n",
        "       print_security_analysis(risks, recommendations)\n",
        "\n",
        "       # Dataset detection\n",
        "       detect_dataset_comprehensive(model_path.name, {})\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Error analyzing sklearn model: {e}\")\n",
        "\n",
        "def analyze_sklearn_complexity(model):\n",
        "   \"\"\"Analyze sklearn model complexity\"\"\"\n",
        "   complexity = {}\n",
        "   model_type = type(model).__name__\n",
        "\n",
        "   # Memory complexity\n",
        "   model_size = len(pickle.dumps(model)) / 1024  # KB\n",
        "   complexity['Model Size'] = f\"{model_size:.1f} KB\"\n",
        "\n",
        "   # Time complexity estimates\n",
        "   if hasattr(model, 'n_estimators'):\n",
        "       n_est = model.n_estimators\n",
        "       complexity['Time Complexity'] = f\"O(n_estimators * tree_depth) ≈ O({n_est} * depth)\"\n",
        "   elif 'SVM' in model_type:\n",
        "       complexity['Time Complexity'] = \"O(n_support_vectors * n_features)\"\n",
        "   elif 'Linear' in model_type:\n",
        "       complexity['Time Complexity'] = \"O(n_features)\"\n",
        "   else:\n",
        "       complexity['Time Complexity'] = \"Model-dependent\"\n",
        "\n",
        "   # Feature scaling requirements\n",
        "   if 'SVM' in model_type or 'Logistic' in model_type:\n",
        "       complexity['Feature Scaling'] = \"Required\"\n",
        "   elif 'Tree' in model_type or 'Forest' in model_type:\n",
        "       complexity['Feature Scaling'] = \"Not required\"\n",
        "   else:\n",
        "       complexity['Feature Scaling'] = \"Recommended\"\n",
        "\n",
        "   return complexity\n",
        "\n",
        "def main():\n",
        "   \"\"\"Enhanced main function with system info\"\"\"\n",
        "   print(\"COMPREHENSIVE AI MODEL ANALYZER\")\n",
        "   print(\"Advanced analysis for AI developers\")\n",
        "   print(\"=\"*80)\n",
        "\n",
        "   # System information\n",
        "   print(f\"System Info:\")\n",
        "   print(f\"  Python Version:    {sys.version.split()[0]}\")\n",
        "   print(f\"  Available RAM:     {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
        "   print(f\"  Available Disk:    {psutil.disk_usage('/').free / (1024**3):.1f} GB\")\n",
        "\n",
        "   try:\n",
        "       import GPUtil\n",
        "       gpus = GPUtil.getGPUs()\n",
        "       if gpus:\n",
        "           print(f\"  GPU Available:     {gpus[0].name} ({gpus[0].memoryTotal}MB)\")\n",
        "       else:\n",
        "           print(f\"  GPU Available:     None detected\")\n",
        "   except:\n",
        "       print(f\"  GPU Available:     Unknown\")\n",
        "\n",
        "   print()\n",
        "\n",
        "   # Find models\n",
        "   content_dir = Path(\"/content\")\n",
        "   if not content_dir.exists():\n",
        "       content_dir = Path(\".\")\n",
        "       print(\"Using current directory (not in Colab environment)\")\n",
        "\n",
        "   extensions = ['.onnx', '.h5', '.hdf5', '.pt', '.pth', '.bin', '.pkl', '.joblib']\n",
        "   models = []\n",
        "\n",
        "   print(f\"Scanning {content_dir} for AI models...\")\n",
        "   for ext in extensions:\n",
        "       found = list(content_dir.glob(f\"*{ext}\"))\n",
        "       models.extend(found)\n",
        "       if found:\n",
        "           print(f\"  Found {len(found)} {ext} files\")\n",
        "\n",
        "   if not models:\n",
        "       print(\"\\nNo model files found!\")\n",
        "       print(f\"Supported formats: {', '.join(extensions)}\")\n",
        "       print(\"\\nTip: Use the Hugging Face downloader script first\")\n",
        "       return\n",
        "\n",
        "   # Remove duplicates and sort by size\n",
        "   unique_models = sorted(list(set(models)), key=lambda x: x.stat().st_size, reverse=True)\n",
        "\n",
        "   print(f\"\\nFound {len(unique_models)} model file(s) (sorted by size):\")\n",
        "   total_size = 0\n",
        "   for i, model in enumerate(unique_models):\n",
        "       size_mb = model.stat().st_size / (1024 * 1024)\n",
        "       total_size += size_mb\n",
        "       age_days = (time.time() - model.stat().st_mtime) / (24 * 3600)\n",
        "       print(f\"  {i+1:2}. {model.name:<40} {size_mb:>8.2f} MB  ({age_days:>3.0f} days old)\")\n",
        "\n",
        "   print(f\"\\nTotal model storage: {total_size:.2f} MB\")\n",
        "\n",
        "   # Select model\n",
        "   if len(unique_models) == 1:\n",
        "       selected = unique_models[0]\n",
        "       print(f\"\\nAuto-selected: {selected.name}\")\n",
        "   else:\n",
        "       print(f\"\\nAnalysis options:\")\n",
        "       print(f\"  0. Analyze ALL models\")\n",
        "       for i, model in enumerate(unique_models):\n",
        "           print(f\"  {i+1}. {model.name}\")\n",
        "\n",
        "       try:\n",
        "           choice = input(f\"\\nSelect option (0-{len(unique_models)}): \").strip()\n",
        "           if choice == '0':\n",
        "               # Analyze all models\n",
        "               for model in unique_models:\n",
        "                   print(f\"\\n\" + \"=\"*100)\n",
        "                   print(f\"ANALYZING: {model.name}\")\n",
        "                   print(\"=\"*100)\n",
        "                   analyze_model_file(model)\n",
        "               return\n",
        "           else:\n",
        "               choice = int(choice) - 1\n",
        "               if 0 <= choice < len(unique_models):\n",
        "                   selected = unique_models[choice]\n",
        "               else:\n",
        "                   selected = unique_models[0]\n",
        "                   print(f\"Invalid choice, using: {selected.name}\")\n",
        "       except (ValueError, KeyboardInterrupt):\n",
        "           selected = unique_models[0]\n",
        "           print(f\"Using first model: {selected.name}\")\n",
        "\n",
        "   print(f\"\\nStarting comprehensive analysis of: {selected.name}\")\n",
        "   print(\"=\"*80)\n",
        "\n",
        "   # Analyze the selected model\n",
        "   analyze_model_file(selected)\n",
        "\n",
        "   print(f\"\\n{'='*80}\")\n",
        "   print(\"ANALYSIS COMPLETE - Ready for production deployment!\")\n",
        "   print(f\"{'='*80}\")\n",
        "\n",
        "   # Final recommendations\n",
        "   print(f\"\\nNEXT STEPS FOR AI DEVELOPERS:\")\n",
        "   print(\"1. Review security recommendations above\")\n",
        "   print(\"2. Test model with sample inputs\")\n",
        "   print(\"3. Benchmark performance on target hardware\")\n",
        "   print(\"4. Implement monitoring and logging\")\n",
        "   print(\"5. Plan model versioning strategy\")\n",
        "   print(\"6. Document API interfaces and data formats\")\n",
        "\n",
        "# Additional imports for enhanced functionality\n",
        "try:\n",
        "   import psutil\n",
        "   import sys\n",
        "except ImportError:\n",
        "   print(\"Installing additional dependencies...\")\n",
        "   import subprocess\n",
        "   subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'psutil', '-q'])\n",
        "   import psutil\n",
        "   import sys\n",
        "\n",
        "# Run the enhanced analyzer\n",
        "if __name__ == \"__main__\":\n",
        "   main()\n",
        ""
      ]
    }
  ]
}